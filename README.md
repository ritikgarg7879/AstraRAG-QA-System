ğŸ“„ DocuMind AI â€“ Intelligent Document Q&A System (RAG-based)
============================================================

DocuMind AI is an **AI-powered document question-answering system** built using **Retrieval-Augmented Generation (RAG)**.It allows users to upload documents and ask questions, and the system answers **strictly based on the document content**, ensuring **accuracy, grounding, and transparency**.

This project demonstrates **real-world GenAI system design**, combining **LLMs, vector databases, agents, and APIs**.

ğŸš€ Features
-----------

*   ğŸ“š Upload and process documents (PDF / text-based files)
    
*   ğŸ” Semantic search using embeddings and vector database
    
*   ğŸ¤– AI-generated answers grounded in document content
    
*   ğŸ§  Multi-turn conversational memory
    
*   ğŸ“‘ Source transparency (answers are backed by retrieved chunks)
    
*   âš¡ Fast and scalable backend using FastAPI
    
*   ğŸ¨ Simple and interactive UI using Streamlit
    

ğŸ§  Why Retrieval-Augmented Generation (RAG)?
--------------------------------------------

Large Language Models (LLMs) are powerful but:

*   They **hallucinate**
    
*   They **donâ€™t know private documents**
    
*   They **cannot be trusted blindly**
    

**RAG solves this by:**

1.  Retrieving **relevant document content**
    
2.  Feeding it to the LLM as **context**
    
3.  Generating answers **only from retrieved data**
    

This ensures:

*   âœ… Accuracy
    
*   âœ… Trustworthiness
    
*   âœ… No hallucinations
    

ğŸ—ï¸ System Architecture
-----------------------

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   User â”‚ â”‚ Question â–¼Streamlit UI â”‚ â–¼FastAPI Backend â”‚ â”œâ”€â”€ Document Loader â”‚ â”œâ”€â”€ Chunking Engine â”‚ â”œâ”€â”€ Embedding Generator (HuggingFace) â”‚ â”œâ”€â”€ Vector Store (ChromaDB) â”‚ â”œâ”€â”€ Retriever (Semantic Search) â”‚ â””â”€â”€ CrewAI Agent        â”‚        â–¼     Groq LLM        â”‚        â–¼   Final Answer + Sources   `

ğŸ› ï¸ Tech Stack
--------------

LayerTechnologyBackend APIFastAPIFrontendStreamlitAI AgentCrewAIRAG FrameworkLlamaIndexEmbeddingsHuggingFaceVector DatabaseChromaDBLLM ProviderGroqLanguagePythonVersion ControlGitHub

ğŸ“‚ Project Structure
--------------------

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   DocuMind-AI/â”‚â”œâ”€â”€ backend/â”‚   â”œâ”€â”€ main.py              # FastAPI entry pointâ”‚   â”œâ”€â”€ rag_pipeline.py      # RAG logicâ”‚   â”œâ”€â”€ crew_agent.py        # CrewAI agent definitionâ”‚   â”œâ”€â”€ embeddings.py        # HuggingFace embedding setupâ”‚   â”œâ”€â”€ vector_store.py      # ChromaDB integrationâ”‚   â””â”€â”€ utils.py             # Helper functionsâ”‚â”œâ”€â”€ frontend/â”‚   â””â”€â”€ app.py               # Streamlit UIâ”‚â”œâ”€â”€ data/â”‚   â””â”€â”€ uploads/             # Uploaded documentsâ”‚â”œâ”€â”€ chroma_db/               # Persistent vector databaseâ”‚â”œâ”€â”€ requirements.txtâ”œâ”€â”€ .env.exampleâ””â”€â”€ README.md   `

ğŸ”„ End-to-End Flow (How Everything Works)
-----------------------------------------

### 1ï¸âƒ£ Document Upload

*   User uploads a document from Streamlit UI
    
*   Document is sent to FastAPI backend
    

### 2ï¸âƒ£ Document Chunking

*   Large documents are split into **smaller chunks**
    
*   Chunk size: ~1024 tokens with overlap
    
*   This improves:
    
    *   Retrieval accuracy
        
    *   Context relevance
        

### 3ï¸âƒ£ Embedding Generation

*   Each chunk is converted into a **vector embedding**
    
*   Uses **HuggingFace sentence transformer models**
    
*   Embeddings capture **semantic meaning**, not keywords
    

### 4ï¸âƒ£ Vector Storage (ChromaDB)

*   Embeddings are stored in **ChromaDB**
    
*   Persistent storage enables:
    
    *   Faster future queries
        
    *   No need to re-embed documents
        

### 5ï¸âƒ£ Semantic Retrieval

*   User question is converted into an embedding
    
*   ChromaDB performs **similarity search**
    
*   Top-k most relevant chunks are retrieved
    

### 6ï¸âƒ£ CrewAI Agent Reasoning

*   Retrieved chunks are passed to a **CrewAI agent**
    
*   Agent responsibilities:
    
    *   Read retrieved content
        
    *   Stay strictly grounded to documents
        
    *   Structure a clean, understandable answer
        

### 7ï¸âƒ£ LLM Response Generation

*   Agent uses **Groq-powered LLM**
    
*   LLM generates answer **only using provided context**
    
*   No external or hallucinated data
    

### 8ï¸âƒ£ Answer + Sources

*   Final answer is sent to frontend
    
*   Supporting document chunks are shown as **sources**
    
*   Ensures transparency and trust
    

ğŸ¤– CrewAI Agent Design
----------------------

The CrewAI agent is responsible for:

*   Context understanding
    
*   Answer structuring
    
*   Preventing hallucinations
    

**Agent Instructions (Conceptually):**

*   Answer only from retrieved documents
    
*   If answer not found, say _â€œInformation not available in the documentâ€_
    
*   Be concise and clear
    

ğŸ” Environment Variables
------------------------

Create a .env file:

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   GROQ_API_KEY=your_groq_api_key   `

â–¶ï¸ How to Run the Project
-------------------------

### 1ï¸âƒ£ Clone Repository

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   git clone https://github.com/your-username/DocuMind-AI.gitcd DocuMind-AI   `

### 2ï¸âƒ£ Create Virtual Environment

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   python -m venv venvsource venv/bin/activate   # Windows: venv\Scripts\activate   `

### 3ï¸âƒ£ Install Dependencies

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   pip install -r requirements.txt   `

### 4ï¸âƒ£ Start Backend (FastAPI)

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   cd backenduvicorn main:app --reload   `

Backend runs at:

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   http://localhost:8000   `

### 5ï¸âƒ£ Start Frontend (Streamlit)

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   cd frontendstreamlit run app.py   `

Frontend runs at:

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   http://localhost:8501   `

ğŸ§ª Example Use Case
-------------------

1.  Upload a company policy PDF
    
2.  Ask: _â€œWhat is the leave policy?â€_
    
3.  System retrieves relevant policy sections
    
4.  AI answers with exact policy explanation
    
5.  Source text is displayed for verification
    
    

ğŸ¯ What This Project Demonstrates
---------------------------------

*   Real-world **RAG architecture**
    
*   Vector databases & semantic search
    
*   AI agent orchestration
    
*   Backendâ€“Frontend integration
    
*   Production-style AI system design
    

ğŸ§‘â€ğŸ’» Author
------------

**Ritik Garg**
